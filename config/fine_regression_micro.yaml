run_name: fine_regression_micro_${net.arch}
experiment: RegressionExperiment

backbone: runs/pretraining_micro/2024-06-06_22-41-11 # best: ema + adam + norm + l2
latent_dim: 128

net:
  units:
  - ${latent_dim}
  - 256
  - 256
  - 6
  out_act: sigmoid

data:
  dir: /remote/gpu01a/heneka/21cmlightcones/pure_simulations/x5
  file_by_file: True

training:
  epochs: 120
  lr: 1.e-3
  batch_size: 64
  optimizer:
    name: AdamW
    kwargs: {weight_decay: 1.e-3}
  
defaults:
  - default
  - preprocessing: xandy
  - net: mlp
  - _self_