run_name: pretraining_mini_sched
experiment: PretrainingExperiment
evaluate: False
plot: False

model: JEPA
ema_momentum: 0.9997
momentum_schedule: False
sim: l1
augment: False
norm_target: False

net: # encoders
  use_mask_token: False
  use_head: False
  hidden_dim: 144
  depth: 4

predictor:
  use_head: False
  hidden_dim: 48
  depth: 4
  learn_pos_encoding: False
  in_dim: ${net.hidden_dim}
  in_shape: ${net.in_shape}
  patch_shape: ${net.patch_shape}

data:
  dir: /scratch/nordmann/data_new/x2
  #dir: /remote/gpu02/ore/data/x2
  file_by_file: False
  sequential_splits: True
  splits: {train: 0.95, val: 0.05, test: 0.}

cluster:
  queue: h100
  node: 1
  mem: 600gb # 21k lightcones = 462gb

training:
  epochs: 1200
  lr: 0.001
  batch_size: 64
  save_best_epoch: True
  save_freq: 75
  optimizer:
    name: AdamW
    kwargs: {weight_decay: 0.001}
  scheduler:
    name: OneCycleLR
    kwargs: {max_lr: 0.001}

defaults:
  - default
  - masking: multiblock
  - net: vit_mini
  - net@predictor: vit_mini
  - preprocessing: xonly  
  - _self_