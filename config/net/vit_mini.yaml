arch: ViT
in_shape: [1, 24, 24, 395]
out_channels: 6
in_conv: False
patch_shape: [7, 7, 25] # 10 x 10 x 45 = 4500 tokens with 7x7x25 = 1225 voxels
mask_frac: 0.
# multiple of 6
hidden_dim: 240
depth: 2
num_heads: 4
mlp_ratio: 2.0
attn_drop: 0.
proj_drop: 0.
mlp_drop: 0.25
learn_pos_encoding: True
long_skips: False
out_act: sigmoid
out_mlp: True
checkpoint_grads: False


# experiment with: patch_shape, hidden_dim, depth, mlp_ratio, num_heads?, learn_pos_encoding? 
# dropout for overfitting
# hidden dim in order of voxels
# 1) hidden dim ratio to patch shape 2) depth
