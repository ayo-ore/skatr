arch: ViT
in_shape: [1, 70, 70, 1175]
out_channels: 6
# embedding size?
# patch_shape: [7, 7, 47] # 10 x 10 x 25 = 2500 tokens with 7x7x47 = 2300 voxels
#patch_shape: [7, 7, 25] # 10 x 10 x 45 = 4500 tokens with 7x7x25 = 1225 voxels
patch_shape: [7, 7, 25]
#patchx: 7
#patchz: 25
#patch_shape: [patchx, patchx, patchz]
# condition_dim: 46 
# multiple of 6
# embedding size?
#hidden_dim: 240
# /2 /2 /5 = /20 as patches 
hidden_dim: 240
depth: 2
num_heads: 4
mlp_ratio: 2.0
attn_drop: 0.
# up to .5
proj_drop: 0.
learn_pos_encoding: True
# final_conv: False
# final_conv_channels: None
long_skips: False
out_act: sigmoid
out_mlp: False

# experiment with: patch_shape, hidden_dim, depth, mlp_ratio, num_heads?, learn_pos_encoding? 
# dropout for overfitting
# hidden dim in order of voxels
# 1) hidden dim ratio to patch shape 2) depth