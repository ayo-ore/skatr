arch: ViT
in_shape: [1, 70, 70, 1175]
out_channels: 6
<<<<<<< HEAD
# embedding size
patch_shape: [7, 7, 47]
condition_dim: 46
# multiple of 6, started at 240
hidden_dim: 360
# started at 4
depth: 8
num_heads: 8
=======
# patch_shape: [7, 7, 47] # 10 x 10 x 25 = 2500 tokens with 7x7x47 = 2300 voxels
patch_shape: [7, 7, 25] # 10 x 10 x 45 = 4500 tokens with 7x7x47 = 1225 voxels
# condition_dim: 46 
hidden_dim: 240
depth: 2
num_heads: 4
>>>>>>> main
mlp_ratio: 2.0
attn_drop: 0.
proj_drop: 0.
learn_pos_encoding: True
# final_conv: False
# final_conv_channels: None
long_skips: False
out_act: sigmoid

# experiment with: patch_shape, hidden_dim, depth, num_heads?, learn_pos_encoding? 