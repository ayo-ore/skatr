#run_name: regression_mini_${net.arch}_P${net.patchx}x${net.patchz}:H${net.hidden_dim}:D${net.depth}:Hs${net.num_heads}
run_name: siam_mini_${net.arch}:debug #_H$:drop_test #{net.hidden_dim}:D${net.depth}:Hs${net.num_heads} #:drop${net.proj_drop}:out_mlp #:big_zpatch
experiment: SimSiamExperiment
# 6 arbitrary (depending on ViT implementation)
# TODO pass to ViT
latent_dim: 6
net:
  out_channels: ${latent_dim}

data:
  # not stored in gpu:
  # also use cluster.node=1 (no preferred gpu)
  dir: /remote/gpu01a/heneka/21cmlightcones/pure_simulations/x2
  # stored gpu01:
  #dir: /scratch2/heneka/21cmlightcones/pure_simulations/x2
  # stored gpu02:
  #dir: /scratch/ore/data/x2
  # in pbs.yaml node: gpu02
  # increasingly worse with growing epochs
  file_by_file: True

preprocessing:
  #AddSingletonChannel: {}
  #Center: {}
  AddSingletonChannelXOnly: {}
  CenterXOnly: {}

training:
  epochs: 15
  lr: 0.0001
  batch_size: 32

predictor:
  # 100 and numer of layers arbitrary (depending on ViT implementation)
  # bottleneck in reference to latent dim
  units:
    - ${latent_dim}
    - 100
    - 100
    - ${latent_dim}
  act: relu
  out_act: null
  drop: True
  
defaults:
  - default
  - net: vit_mini
  - _self_